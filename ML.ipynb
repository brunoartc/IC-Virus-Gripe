{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def get_dataframes():\n",
    "    dataframes=pd.read_excel(\"com_Nas_por_campo_Todas_Tabelas_Flu_de_2010_a_2019_com_os_63_campos_da_Intersecao_de_Todos_e 244774_Amostras.xlsx\")\n",
    "    dataframes[\"NT_DT\"] = pd.to_datetime(dataframes['DT_NOTIFIC'], errors = 'coerce')\n",
    "    dataframes[\"NT_DT\"] = dataframes['NT_DT'].map(lambda x: x.year)\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_uf(uf):\n",
    "    if uf == \"RO\":\n",
    "        return 11\n",
    "    elif uf == \"AC\":\n",
    "        return 12\n",
    "    elif uf == \"AM\":\n",
    "        return 13\n",
    "    elif uf == \"RR\":\n",
    "        return 14\n",
    "    elif uf == \"PA\":\n",
    "        return 15\n",
    "    elif uf == \"AP\":\n",
    "        return 16\n",
    "    elif uf == \"TO\":\n",
    "        return 17\n",
    "    elif uf == \"MA\":\n",
    "        return 21\n",
    "    elif uf == \"PI\":\n",
    "        return 22\n",
    "    elif uf == \"CE\":\n",
    "        return 23\n",
    "    elif uf == \"RN\":\n",
    "        return 24\n",
    "    elif uf == \"PB\":\n",
    "        return 25\n",
    "    elif uf == \"PE\":\n",
    "        return 26\n",
    "    elif uf == \"AL\":\n",
    "        return 27\n",
    "    elif uf == \"SE\":\n",
    "        return 28\n",
    "    elif uf == \"BA\":\n",
    "        return 29\n",
    "    elif uf == \"MG\":\n",
    "        return 31\n",
    "    elif uf == \"ES\":\n",
    "        return 32\n",
    "    elif uf == \"RJ\":\n",
    "        return 33\n",
    "    elif uf == \"SP\":\n",
    "        return 35\n",
    "    elif uf == \"PR\":\n",
    "        return 41\n",
    "    elif uf == \"SC\":\n",
    "        return 42\n",
    "    elif uf == \"RS\":\n",
    "        return 43\n",
    "    elif uf == \"MS\":\n",
    "        return 50\n",
    "    elif uf == \"MT\":\n",
    "        return 51\n",
    "    elif uf == \"GO\":\n",
    "        return 52\n",
    "    elif uf == \"DF\":\n",
    "        return 53\n",
    "    else:\n",
    "        return int(uf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_regions(df):\n",
    "    regions = [[\"53\",\"52\",\"50\",\"51\"],[\"16\",\"12\",\"13\",\"15\",\"11\",\"14\",\"17\"],[\"27\",\"29\",\"23\",\"21\",\"25\",\"22\",\"26\",\"24\",\"28\"],[\"32\",\"31\",\"33\",\"35\"],[\"43\",\"42\",\"41\"]]\n",
    "\n",
    "    df['region-centro-oeste'] = df[\"UF_NOT\"].isin(regions[0]) #centro-oeste\n",
    "    df['region-norte'] = df[\"UF_NOT\"].isin(regions[1]) #norte\n",
    "    df['region-nordeste'] = df[\"UF_NOT\"].isin(regions[2]) #nordeste\n",
    "    df['region-sudeste'] = df[\"UF_NOT\"].isin(regions[3]) #nordeste\n",
    "    df['region-sul'] = df[\"UF_NOT\"].isin(regions[4]) #nordeste\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan_values(df, variable):\n",
    "    result_df = df.copy()\n",
    "    result_df[variable] = result_df[variable].fillna(result_df[variable].mode()[0])\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_rows(df, variable):\n",
    "    result_df = df.copy()\n",
    "    result_df[variable] = result_df.sample(frac = 1.0).copy()[variable].values\n",
    "    return result_df.sample(frac = 1.0).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import timedelta\n",
    "def clean_df(df):\n",
    "    # removing NaN\n",
    "    #df_clean = df[(df['EVOLUCAO'].notna() & df['SG_UF_NOT'].notna() & df['VACINA'].notna() & df['ANTIVIRAL'].notna() & df['PNEUMOPATI'].notna() & df['CARDIOPATI'].notna())].copy()\n",
    "    fill_na_fileds = ['EVOLUCAO', 'SG_UF_NOT', 'VACINA', 'ANTIVIRAL', 'PNEUMOPATI', 'CARDIOPATI']\n",
    "    for i in fill_na_fileds:\n",
    "        df = fill_nan_values(df, i)\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    df_clean['MORTE'] = df_clean['EVOLUCAO'].map(lambda x: 1 if x == \"2\" or x == \"4\" or x == 2.0 or x == 4.0 else 0)\n",
    "    df_clean['IDADE'] = (pd.to_datetime(df_clean['DT_NOTIFIC'], errors = 'coerce') - pd.to_datetime(df_clean['DT_NASC'], errors = 'coerce')) / timedelta(days=365)\n",
    "    df_clean['UF_NOT'] = df_clean['SG_UF_NOT'].map(lambda x: convert_uf(x))\n",
    "    df_clean['CS_SEXO'] = df_clean['CS_SEXO'].map(lambda x: 1 if x == \"F\" else 2)\n",
    "    label_regions(df_clean)\n",
    "    return df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframes(df):\n",
    "    sensitive_vars = [\"CS_SEXO\", \"IDADE\", \"region-centro-oeste\", \"region-norte\", \"region-nordeste\", \"region-sudeste\", \"region-sul\", \"VACINA\", \"MORTE\", \"ANTIVIRAL\", \"PNEUMOPATI\", \"CARDIOPATI\"]\n",
    "    dataframes = []\n",
    "    dff = clean_df(df)\n",
    "    dates = [2010.0, 2011.0, 2012.0, 2013.0, 2014.0, 2015.0, 2016.0, 2017.0, 2018.0, 2019.0]\n",
    "    for i in dates:\n",
    "        dataframes.append(dff[dff[\"NT_DT\"] == i][sensitive_vars])\n",
    "    dataframes[0] = dataframes[1]\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = get_dataframes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = load_dataframes(raw_df)\n",
    "df = df_processed[5].dropna() #TODO check with 5 only e use complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rf(df, variable):\n",
    "    # Use numpy to convert to arrays\n",
    "    import numpy as np\n",
    "    # mortes are the values we want to predict\n",
    "    mortes = np.array(df['MORTE'])\n",
    "    # Remove the mortes from the features\n",
    "    # axis 1 refers to the columns\n",
    "    df_drop= df.drop('MORTE', axis = 1)\n",
    "    # Saving feature names for later use\n",
    "    feature_list = list(df_drop.columns)\n",
    "    # Convert to numpy array\n",
    "    df_np = np.array(df_drop)\n",
    "    train_features, test_features, train_mortes, test_mortes = train_test_split(df_np, mortes, test_size = 0.20, random_state = 42)\n",
    "\n",
    "    train_f_features, cross_features, train_f_mortes, cross_mortes = train_test_split(train_features, train_mortes, test_size = 0.20, random_state = 42)\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    # Instantiate model with 1000 decision trees\n",
    "    rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "    # Train the model on training data\n",
    "    rf.fit(train_f_features, train_f_mortes);\n",
    "    importances = list(rf.feature_importances_) #problemas com o feature importance \n",
    "    # gerar com a random forest que ele mesmo gerou causa um problema, \n",
    "    #usar uma melhoria \n",
    "    #(1). melhorar o missing data, fazer a media por coluna, (valor que é mais comum)\n",
    "\n",
    "    #tende a ser enviezado por algumas variavies, tem mais homens que mulheres (dar mais importancia para um) [diminui o binario]\n",
    "    #\n",
    "    # permutação - tira cada um e calcula o feature importance depois faz uma outra permutacao e adiciona\n",
    "\n",
    "    # faz uma permutacao e calcula, so da coluna da variavel \n",
    "\n",
    "\n",
    "    # List of tuples with variable and importance\n",
    "\n",
    "    # fazer o random shuffeling e tirar a media da disposição de todos, esse sera o \n",
    "    # (2). descobrir o feature importance \"random permutation\" \n",
    "    feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "    # Sort the feature importances by most important first\n",
    "    feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "    \n",
    "    # Print out the feature and importances \n",
    "    #[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "    for i in feature_importances:\n",
    "        if i[0] == variable:\n",
    "            print(\"shuffle {} {}\".format(variable, i[1]))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle IDADE 0.7\n",
      "shuffle IDADE 0.71\n",
      "shuffle IDADE 0.69\n",
      "shuffle IDADE 0.7\n",
      "shuffle IDADE 0.69\n",
      "shuffle IDADE 0.7\n",
      "shuffle IDADE 0.69\n",
      "shuffle VACINA 0.06\n",
      "shuffle VACINA 0.06\n",
      "shuffle VACINA 0.06\n",
      "shuffle VACINA 0.06\n",
      "shuffle VACINA 0.06\n",
      "shuffle VACINA 0.06\n",
      "shuffle VACINA 0.07\n",
      "shuffle ANTIVIRAL 0.05\n",
      "shuffle ANTIVIRAL 0.05\n",
      "shuffle ANTIVIRAL 0.05\n",
      "shuffle ANTIVIRAL 0.05\n",
      "shuffle ANTIVIRAL 0.05\n",
      "shuffle ANTIVIRAL 0.05\n",
      "shuffle ANTIVIRAL 0.05\n",
      "shuffle CARDIOPATI 0.04\n",
      "shuffle CARDIOPATI 0.04\n",
      "shuffle CARDIOPATI 0.04\n",
      "shuffle CARDIOPATI 0.04\n",
      "shuffle CARDIOPATI 0.04\n",
      "shuffle CARDIOPATI 0.04\n",
      "shuffle CARDIOPATI 0.04\n",
      "shuffle CS_SEXO 0.04\n",
      "shuffle CS_SEXO 0.04\n",
      "shuffle CS_SEXO 0.04\n",
      "shuffle CS_SEXO 0.04\n",
      "shuffle CS_SEXO 0.04\n",
      "shuffle CS_SEXO 0.04\n",
      "shuffle CS_SEXO 0.04\n",
      "shuffle PNEUMOPATI 0.04\n",
      "shuffle PNEUMOPATI 0.04\n",
      "shuffle PNEUMOPATI 0.05\n",
      "shuffle PNEUMOPATI 0.04\n",
      "shuffle PNEUMOPATI 0.05\n",
      "shuffle PNEUMOPATI 0.05\n",
      "shuffle PNEUMOPATI 0.04\n",
      "shuffle region-centro-oeste 0.02\n",
      "shuffle region-centro-oeste 0.02\n",
      "shuffle region-centro-oeste 0.02\n",
      "shuffle region-centro-oeste 0.02\n",
      "shuffle region-centro-oeste 0.02\n",
      "shuffle region-centro-oeste 0.02\n",
      "shuffle region-centro-oeste 0.02\n",
      "shuffle region-sudeste 0.04\n",
      "shuffle region-sudeste 0.04\n",
      "shuffle region-sudeste 0.04\n",
      "shuffle region-sudeste 0.04\n",
      "shuffle region-sudeste 0.04\n",
      "shuffle region-sudeste 0.04\n",
      "shuffle region-sudeste 0.04\n",
      "shuffle region-sul 0.04\n",
      "shuffle region-sul 0.04\n",
      "shuffle region-sul 0.04\n",
      "shuffle region-sul 0.04\n",
      "shuffle region-sul 0.04\n",
      "shuffle region-sul 0.04\n",
      "shuffle region-sul 0.03\n",
      "shuffle region-norte 0.01\n",
      "shuffle region-norte 0.01\n",
      "shuffle region-norte 0.01\n",
      "shuffle region-norte 0.01\n",
      "shuffle region-norte 0.01\n",
      "shuffle region-norte 0.01\n",
      "shuffle region-norte 0.01\n",
      "shuffle region-nordeste 0.03\n",
      "shuffle region-nordeste 0.02\n",
      "shuffle region-nordeste 0.03\n",
      "shuffle region-nordeste 0.03\n",
      "shuffle region-nordeste 0.03\n",
      "shuffle region-nordeste 0.03\n",
      "shuffle region-nordeste 0.03\n"
     ]
    }
   ],
   "source": [
    "variables_for_shuffeling = [\"IDADE\", \"VACINA\", \"ANTIVIRAL\", \"CARDIOPATI\", \"CS_SEXO\", \"PNEUMOPATI\", \"region-centro-oeste\", \"region-sudeste\", \"region-sul\", \"region-norte\", \"region-nordeste\"]\n",
    "\n",
    "\n",
    "\n",
    "for i in variables_for_shuffeling:#TODO check\n",
    "    for _ in range(7):\n",
    "        df_shuffeled = shuffle_rows(df, i).copy()\n",
    "        #print(df_shuffeled[i].head()) tah fazendo o shuffle sim\n",
    "        generate_rf(df_shuffeled, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use numpy to convert to arrays\n",
    "import numpy as np\n",
    "# mortes are the values we want to predict\n",
    "mortes = np.array(df['MORTE'])\n",
    "# Remove the mortes from the features\n",
    "# axis 1 refers to the columns\n",
    "df_drop= df.drop('MORTE', axis = 1)\n",
    "# Saving feature names for later use\n",
    "feature_list = list(df_drop.columns)\n",
    "# Convert to numpy array\n",
    "df_np = np.array(df_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_mortes, test_mortes = train_test_split(df_np, mortes, test_size = 0.20, random_state = 42)\n",
    "\n",
    "train_f_features, cross_features, train_f_mortes, cross_mortes = train_test_split(train_features, train_mortes, test_size = 0.20, random_state = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mortes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9230, 11)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1846, 11)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_features.shape #deixar o validation fixo\n",
    "#(3). cross validation com 5 separações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(train_f_features, train_f_mortes);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: IDADE                Importance: 0.7\n",
      "Variable: VACINA               Importance: 0.05\n",
      "Variable: ANTIVIRAL            Importance: 0.05\n",
      "Variable: CARDIOPATI           Importance: 0.05\n",
      "Variable: CS_SEXO              Importance: 0.04\n",
      "Variable: PNEUMOPATI           Importance: 0.04\n",
      "Variable: region-centro-oeste  Importance: 0.02\n",
      "Variable: region-sudeste       Importance: 0.02\n",
      "Variable: region-sul           Importance: 0.02\n",
      "Variable: region-norte         Importance: 0.01\n",
      "Variable: region-nordeste      Importance: 0.01\n",
      "IDADE\n"
     ]
    }
   ],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_) #problemas com o feature importance \n",
    "# gerar com a random forest que ele mesmo gerou causa um problema, \n",
    "#usar uma melhoria \n",
    "#(1). melhorar o missing data, fazer a media por coluna, (valor que é mais comum)\n",
    "\n",
    "#tende a ser enviezado por algumas variavies, tem mais homens que mulheres (dar mais importancia para um) [diminui o binario]\n",
    "#\n",
    "# permutação - tira cada um e calcula o feature importance depois faz uma outra permutacao e adiciona\n",
    "\n",
    "# faz uma permutacao e calcula, so da coluna da variavel \n",
    "\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "\n",
    "# fazer o random shuffeling e tirar a media da disposição de todos, esse sera o \n",
    "# (2). descobrir o feature importance \"random permutation\" \n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n",
    "\n",
    "\n",
    "#!!!!!!! salvar isso numa pasta importante !!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.2 degrees.\n"
     ]
    }
   ],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_mortes)\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.21220268801410108"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(rf, cross_features, cross_mortes,scoring='neg_mean_absolute_error', cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.20690970807496947"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(rf, train_features, train_mortes,scoring='neg_mean_absolute_error', cv=10))\n",
    "\n",
    "#tutorial de como fazer o cross validation\n",
    "# fazer testes para fazer de forma particionada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
